--- TEXTE HORS-TABLE (PAGE 1) ---
Syllabus projet
Année :2024-2025
Enseignant(s) Email(s)
VIDAL Nicolas nvidal@myges.fr
2025-5A-IABD-DRL
Imprimé le : 06/07/25 23:05

--- TEXTE HORS-TABLE (PAGE 2) ---
Descriptif détaillé
Environnements de départ :
- pour tests : Line World
- pour tests : Grid World
- pour tests : TicTacToe versus Random
+ 1 au choix parmi :
- Farkle (solo ou vs Random ou Heuristique)
> https://boardgamearena.com/gamepanel?game=farkle
- LuckyNumbers (vs Random ou Heuristique)
> https://boardgamearena.com/gamepanel?game=luckynumbers
- Pond (versus Random ou Heuristique)
> https://boardgamearena.com/gamepanel?game=pond
Types d'agents à étudier :
- Random
- TabularQLearning (quand possible)
- DeepQLearning
- DoubleDeepQLearning
- DoubleDeepQLearningWithExperienceReplay
- DoubleDeepQLearningWithPrioritizedExperienceReplay
- REINFORCE
- REINFORCE with mean baseline
- REINFORCE with Baseline Learned by a Critic
- PPO A2C style
- RandomRollout
- Monte Carlo Tree Search (UCT)
- Expert Apprentice
- Alpha Zero
- MuZero
- MuZero stochastique
Métriques à obtenir (attention métriques pour la policy obtenue, pas pour la policy en mode entrainement)
:
- Score moyen (pour chaque agent) au bout de 1000 parties d'entrainement
- Score moyen (pour chaque agent) au bout de 10 000 parties d'entrainement
- Score moyen (pour chaque agent) au bout de 100 000 parties d'entrainement
- Score moyen (pour chaque agent) au bout de 1 000 000 parties d'entrainement (si possible)
- Score moyen (pour chaque agent) au bout de XXX parties d'entrainement (si possible)
- Temps moyen mis pour exécuter un coup
Si la partie est de durée variable :
- Longueur moyenne (nombre de step) d'une partie au bout de 1000 parties d'entrainement
- Longueur moyenne (nombre de step) d'une partie au bout de 10 000 parties d'entrainement
- Longueur moyenne (nombre de step) d'une partie au bout de 100 000 parties d'entrainement
- Longueur moyenne (nombre de step) d'une partie au bout de 1 000 000 parties d'entrainement (si
possible)
- Longueur moyenne d'une partie au bout de XXX parties (si possible)
Il sera également nécessaire de présenter une interface graphique permettant de regarder jouer chaque
agent et également de mettre à disposition un agent 'humain'.
Pour chaque environnement et chaque algorithme, les étudiants devront étudier les performances de
l'algorithme et retranscrire leur résultats.
Les étudiants devront fournir l'intégralité du code leur ayant permis d'obtenir leurs résultats ainsi que les
modèles (keras/tensorflow/pytorch/jax/keras_core/burn) entraînés et sauvegardés prêts à être exécutés
pour confirmer les résultats présentés.
Les étudiants devront présenter ces résultats dans un rapport ainsi qu'une présentation. Dans ces
derniers, les étudiants devront faire valoir leur méthodologie de choix d'hyperparamètres, et proposer leur
interprétation des résultats obtenus
Imprimé le : 06/07/25 23:05

--- TEXTE HORS-TABLE (PAGE 3) ---
Ouvrages de référence (livres, articles, revues, sites web...)
Reinforcement Learning: An Introduction de Richard S. Sutton and Andrew G. Barto
Outils informatiques à installer
tensorflow / keras / pytorch / jax / keras_core / burn
Imprimé le : 06/07/25 23:05

